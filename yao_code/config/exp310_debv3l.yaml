dataset:
  path: ...
  fold_file: /home/heyao/kaggle/feedback-ells/input/fb3_folds/train_4folds.csv
  n_samples_in_train: 0

model:
  path: /media/heyao/42f00068-ba8d-48dd-8719-61eda5244b8d/pretrained-models/deberta-v3-large
  type: regression
  pooling: lstm_meanmax
  layer_start: 12
  msd: false
  residual_transformers: false
  num_reinit_layers: 1
  differential_lr:
    enable: true
    lr_factor: 2.6

optim:
  optimizer:
    lookahead: false
    lr: 7e-6
    head_lr: 1e-5
    betas: [0.9, 0.999]
    eps: 1e-6
    weight_decay: 1e-2
  scheduler:
    name: cosine
    num_warmup_steps: 0
    kwargs: {}

train:
  device: gpu
  num_folds: 4
  fold_index: [0]
  random_seed: 42
  ensure_data_order: true
  reinit_weights: true
  loss: mcrmse
  zero_dropout: true
  # whether to use `resolve_encodings_and_normalize`
  clean: false
  accumulate_grad: 1
  max_length: 768
  batch_size: 2
  epochs: 3
  num_workers: 12
  max_grad_norm: 10
  validation_interval: 0.2
  precision: 16
  gradient_checkpointing: true
  freeze_embeddings: false
  freeze_encoders: 0
  trained_target: null

  report_to:
    tensorboard:
      enable: true
      path: /home/heyao/tf-logs
    wandb:
      enable: false

  multi_task:
    enable: false
  mixout:
    enable: false

  awp:
    enable: false
    from_score: 0.48
    adv_param: weight
    adv_lr: 1
    adv_eps: 1e-2
    start_epoch: 1
    adv_step: 1
  ema:
    enable: false

  pl:
    enable: true
    min_dist: false
    path: /home/heyao/kaggle/feedback-english-lan-learning/output/pl/pl_dbv3l_f4_fold{fold}.csv
    stage: 2
    save_last_checkpoint: false
    prev_model_path: /home/heyao/kaggle/feedback-english-lan-learning/output/weights/stage1/deberta-v3-large_regression_fold{fold}.ckpt

tokenizer:
  label_token_ids: [[31066], [16843], [11174], [5741, 8495], [10877], [15521]]
  label_start_id: 128001
  label_end_id: 128006
  label_inject: false
  label_positions: [4, 6, 8, 10, 11, 15]
